# CT Slice Interpolation Training Configuration
# Task: Thick slices (50 @ 5.0mm) → Thin slices (300 @ 1.0mm)
# Architecture: Latent Diffusion with Custom Trained VAE + Medium U-Net (599M params)
# Data: Full volumes (NO patches) - 356 patients

# Model Configuration - MEDIUM (599M params)
model:
  in_channels: 1  # Grayscale CT scans
  latent_dim: 8  # Custom VAE uses 8 latent channels
  vae_base_channels: 128  # Match custom VAE training config
  vae_scaling_factor: 1.0  # Custom VAE scaling factor

  # U-Net Configuration - MEDIUM MODEL (192 channels)
  unet_model_channels: 128  # ↑ from 128 (+50%) → 599M params total
  unet_num_res_blocks: 2
  unet_attention_levels: [1, 2]
  unet_channel_mult: [1, 2, 4, 4]  # Full 4-level U-Net
  unet_num_heads: 8
  unet_time_embed_dim: 1024

  # Diffusion Configuration
  noise_schedule: 'cosine'
  diffusion_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

# Pretrained Weights - Custom Trained VAE
pretrained:
  use_pretrained: true

  # Custom Trained VAE
  vae:
    enabled: true
    checkpoint_path: '/workspace/storage_a100/checkpoints/vae_training_custom_vae_o1/vae_best.pt'
    freeze_epochs: 0  # VAE frozen from start (trained separately)

  # Training strategy
  two_phase_training: false  # Single-phase: U-Net only
  phase1_epochs: 0

  # Layer-wise learning rates
  layer_lr_multipliers:
    vae_encoder: 0.0  # Frozen (pretrained custom VAE)
    vae_decoder: 0.0  # Frozen (pretrained custom VAE)
    unet: 1.0  # Full learning rate

# Data Configuration - PATCH-BASED (Slice Interpolation) for Memory Efficiency
data:
  # Data source for slice interpolation
  data_source: 'slice_interpolation'  # Use slice interpolation loader

  # Local dataset path (PVC mounted)
  dataset_path: '/workspace/storage_a100/dataset'  # APE-data with .zip files

  # IMPORTANT: Preprocessing cache directory (persistent storage!)
  # Pipeline will:
  # 1. Extract ZIPs ONCE
  # 2. Preprocess DICOM → save as .pt tensors
  # 3. DELETE extracted DICOM files (saves ~40-50GB storage!)
  # 4. Keep only preprocessed .pt files (~15-20GB)
  # Subsequent runs load cached .pt files (very fast!)
  extract_dir: '/workspace/storage_a100/.cache/slice_interpolation_cache'
  processed_dir: '/workspace/storage_a100/.cache/processed'  # Preprocessed .pt files

  # Categories
  categories: ['APE', 'non-APE']

  # Patch-based configuration (NEW for memory efficiency!)
  use_patches: true  # Enable patch-based training
  patch_depth_thin: 48   # Number of thin slices in HR patch
  patch_depth_thick: 8   # Number of thick slices in LR patch (6× ratio)
  patch_size: [192, 192] # Spatial patch size (H, W)
  augment: true          # Random flips and rotations (training only)

  # CT windowing
  window_center: 40   # HU for soft tissue
  window_width: 400   # HU for soft tissue + vessels

  # Train/Val/Test split
  val_split: 0.15  # 15% validation (≈53 patients)
  test_split: 0.10  # 10% test (≈36 patients)
  seed: 42

  # DataLoader settings (INCREASED for patches!)
  batch_size: 8  # Moderate batch size for custom VAE + UNet (changed from 16)
  num_workers: 8  # Increased workers for patch sampling
  pin_memory: true
  drop_last: true
  cache_extracted: false  # Not needed - preprocessing pipeline deletes DICOMs automatically

# Training Configuration
training:
  num_epochs: 300  # Increased for diffusion model convergence (was 100)
  learning_rate: 0.0001  # 1e-4
  weight_decay: 0.01

  # Model identification
  model_suffix: 'slice_interp_full3'

  # Optimization
  optimizer: 'adamw'
  gradient_accumulation_steps: 2  # Effective batch = 8*2 = 16 (changed from 1)
  max_grad_norm: 1.0  # Gradient clipping

  # Mixed precision
  mixed_precision: true
  precision: 'bf16'  # BF16 for better stability and speed on A100

  min_lr: 0.000001

  # Checkpointing Strategy
  # Only 2 checkpoints are saved:
  # 1. Best checkpoint: Saved after patch validation if loss improves (tracked automatically)
  # 2. Final checkpoint: Saved at end of training
  # No intermediate checkpoints - saves disk space and keeps checkpoint dir clean

  # Validation Strategy (Multi-Tier)
  # Tier 1: Loss-only validation (every epoch, fast)
  val_interval: 1000  # Steps between loss-only validation (or set high for per-epoch)

  # Tier 2: Patch-based validation with generation (periodic, medium cost)
  patch_val_interval: 5  # Every N epochs - generates predictions on patches
  patch_val_samples: 10  # Reduced from 40 to speed up validation (90% time reduction)
  patch_val_generate: false  # Disabled generation for faster validation (use loss only)

  # Tier 3: Full-volume validation (rare, expensive)
  full_val_interval: 10000  # Every N epochs - full 512×512×300 volume generation
  full_val_samples: 1  # Number of full volumes (very expensive!)

  # Final validation (end of training)
  final_val_enabled: true  # Run comprehensive validation after training
  final_val_full_volumes: true  # Use full volumes for final validation
  # Note: Final validation uses ALL validation samples (no limit)

  # Logging
  log_interval: 100
  output_dir: '/workspace/storage_a100/outputs'
  log_dir: '/workspace/storage_a100/logs'
  checkpoint_dir: '/workspace/storage_a100/checkpoints'
  experiment_name: 'slice_interp_full_medium'

  # Weights & Biases
  use_wandb: false
  wandb_project: 'ct-slice-interpolation'
  wandb_entity: null

# Loss Configuration - Pure MSE Baseline (for custom VAE testing)
losses:
  # Primary loss: Diffusion (MSE on noise prediction)
  use_diffusion_loss: true

  # Auxiliary losses DISABLED for initial baseline (can re-enable later)
  use_perceptual_loss: false  # Disabled for MSE-only baseline (changed from true)
  lambda_perceptual: 0.0  # Weight for VGG perceptual loss (changed from 0.1)
  perceptual_every_n_steps: 10  # Compute every 10 steps (reduce overhead)

  use_ms_ssim_loss: false  # Disabled for MSE-only baseline (changed from true)
  lambda_ssim: 0.0  # Weight for MS-SSIM loss (changed from 0.1)
  ssim_every_n_steps: 10  # Compute every 10 steps

  # Current loss: L_total = L_diffusion (pure MSE)

# Hardware Configuration
hardware:
  device: 'cuda'
  num_gpus: 1
  distributed: false

  # Memory optimization (REQUIRED for full volumes)
  gradient_checkpointing: true  # Enable for 512×512×300 volumes
  cpu_offload: false



# Expected Performance (CT Slice Interpolation with Full Volumes + Medium Model):
#
# 1. MODEL CAPACITY:
#    - U-Net: 599M parameters (2.2× larger than baseline 270M)
#    - VAE: 130M parameters (custom trained)
#    - Total: 729M parameters
#    - Expected improvement: +1-2 dB PSNR over baseline
#
# 2. MEMORY USAGE (A100 80GB):
#    - Input: 2 × (1, 50, 512, 512) = ~0.4 GB thick slices
#    - Target: 2 × (1, 300, 512, 512) = ~2.4 GB thin slices
#    - Latents: 2 × (4, 75, 64, 64) = ~0.1 GB (after VAE encoding)
#    - Model: ~599M params × 2 bytes (fp16) = ~1.2 GB
#    - Activations + gradients: ~24-30 GB (with gradient checkpointing)
#    - Total: ~28-33 GB / 80 GB ✓ SAFE
#
# 3. TRAINING DYNAMICS:
#    - Epoch 1: PSNR ~30-32 dB (good start with pretrained VAE)
#    - Epoch 25: PSNR ~38-40 dB (convergence)
#    - Epoch 50: PSNR ~42-44 dB (with perceptual + SSIM)
#    - Epoch 100: PSNR ~44-46 dB (final quality)
#    - SSIM: 0.92-0.97
#
# 4. DATA CHARACTERISTICS:
#    - 356 training patients (75% of 475 total)
#    - 53 validation patients (15%)
#    - 36 test patients (10%)
#    - Interpolation task: 50 slices → 300 slices (6× depth increase)
#    - Full volumes: NO patches, NO downsampling
#
# 5. TRAINING TIME (A100):
#    - ~5-7 minutes per epoch (356 patients, batch_size=2)
#    - ~8-10 hours for 100 epochs
#    - Early stopping likely around 50-70 epochs
#
# 6. SOTA ALIGNMENT:
#    - ✅ Latent diffusion (3D MedDiffusion 2024)
#    - ✅ Custom trained medical VAE
#    - ✅ Perceptual loss (MSDSR 2024)
#    - ✅ MS-SSIM loss (multi-scale quality)
#    - ✅ Full-volume processing (no artificial downsampling)
#    - ✅ Variable depth handling (50 → 300)
#
# 7. IMPROVEMENTS OVER PREVIOUS APPROACH:
#    - Correct task interpretation (interpolation vs temporal)
#    - Full volumes instead of 24-frame downsampling
#    - 2.2× larger model capacity
#    - Multi-scale loss functions
#    - Better data utilization (356 full patients vs patches)

# Cloud Training Configuration for APE-Data WITH PRETRAINED VAE
# This config is designed to work with Stable Diffusion VAE (latent_dim=8)

# Model Configuration - UPDATED to match SD VAE architecture
model:
  in_channels: 3
  latent_dim: 8  # Changed from 4 to 8 to match SD VAE
  vae_base_channels: 128
  unet_model_channels: 128
  unet_num_res_blocks: 2
  unet_attention_levels: [1, 2]
  unet_channel_mult: [1, 2, 4]
  unet_num_heads: 8
  unet_time_embed_dim: 1024
  noise_schedule: 'cosine'
  diffusion_timesteps: 1000
  beta_start: 0.0001
  beta_end: 0.02

# Pretrained Weights Configuration
pretrained:
  use_pretrained: true

  # VAE pretrained weights
  vae:
    enabled: true
    model_name: 'stabilityai/sd-vae-ft-mse'  # Stable Diffusion VAE
    method: 'auto'
    inflate_method: 'central'  # For 2D->3D inflation
    freeze_epochs: 5  # Freeze VAE for first 5 epochs

  # Training strategy
  two_phase_training: true
  phase1_epochs: 1  # Phase 1: Freeze VAE and train U-Net
  # Phase 2: Fine-tune entire model

  # Layer-wise learning rates
  layer_lr_multipliers:
    vae_encoder: 0.1
    vae_decoder: 0.1
    unet: 1.0

# Data Configuration - HuggingFace Streaming
data:
  data_source: 'huggingface'
  dataset_name: 't2ance/APE-data'
  streaming: true
  cache_dir: null
  max_samples: 5
  categories: ['APE', 'non-APE']
  num_frames: 8
  resolution: [128, 128]
  window_center: 40
  window_width: 400
  batch_size: 1
  num_workers: 1
  pin_memory: true
  drop_last: true
  max_samples: 10

# Training Configuration
training:
  num_epochs: 2
  learning_rate: 0.0001
  weight_decay: 0.01
  optimizer: 'adamw'
  gradient_accumulation_steps: 8
  max_grad_norm: 1.0
  mixed_precision: true
  precision: 'fp16'
  scheduler: 'cosine'
  warmup_steps: 500
  checkpoint_every: 500
  keep_last_n_checkpoints: 1
  validate_every: 1000
  num_validation_samples: 1
  log_every: 50
  output_dir: '/workspace/storage/outputs'
  log_dir: '/workspace/storage/logs'
  checkpoint_dir: '/workspace/storage/checkpoints'
  experiment_name: 'ape_v2v_diffusion_pretrained'
  use_wandb: false
  wandb_project: 'video-diffusion'
  wandb_entity: null

# Hardware Configuration
hardware:
  device: 'cuda'
  num_gpus: 1
  distributed: false
  gradient_checkpointing: true
  cpu_offload: false

# Inference Configuration
inference:
  sampler: 'ddim'
  num_inference_steps: 50
  guidance_scale: 1.0
  save_samples: true
  samples_per_validation: 5

# Resume Training
resume:
  checkpoint_path: null
  resume_optimizer: true
  resume_scheduler: true

# Early Stopping
early_stopping:
  enabled: false
  patience: 10
  metric: 'loss'
  mode: 'min'

# Notes:
# This config uses latent_dim=8 to match Stable Diffusion VAE
# Training with pretrained VAE should converge faster
# First 5 epochs: VAE frozen, only U-Net trains
# Remaining epochs: Full model fine-tuning

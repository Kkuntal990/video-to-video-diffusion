# VAE Training Configuration
# Train a custom 3D VAE from scratch for CT slice interpolation
# Target: PSNR >35 dB reconstruction quality
#
# ⚠️  IMPORTANT: Skip connections are DISABLED for latent diffusion compatibility!
# The VAE must work with encode()/decode() independently (no cross-encoder-decoder skips).

# Model Configuration - Lightweight VAE
model:
  in_channels: 1  # Grayscale CT scans
  latent_dim: 16  # Increased from 8 for better reconstruction (4× compression)
  vae_base_channels: 128  # Base channels for encoder/decoder
  vae_scaling_factor: 1.0  # Standard VAE scaling

  # No pretrained weights - train from scratch
  use_pretrained: false
  use_custom_maisi: false
  use_maisi: false

# Data Configuration - Patch-based for memory efficiency
data:
  data_source: 'slice_interpolation'
  dataset_path: '/workspace/storage_a100/dataset'
  extract_dir: '/workspace/storage_a100/.cache/slice_interpolation_cache'
  processed_dir: '/workspace/storage_a100/.cache/processed'  # Preprocessed .pt files

  categories: ['APE', 'non-APE']

  # Patch-based configuration (NEW!)
  use_patches: true  # Enable patch-based training
  patch_depth_thin: 48   # Number of thin slices in HR patch
  patch_depth_thick: 8   # Number of thick slices in LR patch (6× ratio)
  patch_size: [192, 192] # Spatial patch size (H, W)
  augment: true          # Random flips and rotations (training only)

  # CT windowing
  window_center: 40
  window_width: 400

  # Train/Val/Test split
  val_split: 0.1
  test_split: 0.1
  seed: 42

  # Limit dataset size for debugging (null = use all samples)
  max_train_samples: 20  # Set to 2 for quick debug, null for full training
  max_val_samples: 4    # Limit validation samples (optional)

  # DataLoader settings (increased batch size - skip connections removed!)
  batch_size: 1  # Increased from 1 (skip connections removed = less memory)
  num_workers: 4
  pin_memory: true
  drop_last: true

# VAE Training Configuration
training:
  num_epochs: 100  # VAEs converge quickly
  learning_rate: 0.0001  # 1e-4
  weight_decay: 0.00001

  model_suffix: 'custom_vae_no_skips_overfit'  # No skip connections - for latent diffusion

  # Slice sampling ratio during training
  # Controls how often the VAE trains on thick vs thin slices
  # Range: 0.0 (only thin) to 1.0 (only thick)
  thick_slice_ratio: 0.2  # Default: 20% thick, 80% thin

  # Optimization
  optimizer: 'adamw'
  gradient_accumulation_steps: 4  # No accumulation needed with batch_size=8
  max_grad_norm: 1.0

  # Mixed precision
  mixed_precision: true
  precision: 'bf16'

  # Learning rate scheduling
  scheduler: 'cosine'
  warmup_steps: 500
  min_lr: 0.000001

  # Checkpointing
  checkpoint_every: 1000
  keep_last_n_checkpoints: 3

  # Validation
  val_interval: 5  # Run validation every epoch (was 500 - caused validation to never run!)
  num_validation_samples: 30  # Increased from 10 for better validation statistics

  # Logging
  log_interval: 50
  output_dir: '/workspace/storage_a100/outputs'
  log_dir: '/workspace/storage_a100/logs'
  checkpoint_dir: '/workspace/storage_a100/checkpoints'
  experiment_name: 'vae_training'

  # Weights & Biases
  use_wandb: false
  wandb_project: 'ct-vae-training'

# VAE Loss Configuration
losses:
  # VAE-specific losses
  use_vae_loss: true  # Enable VAE training mode

  # Reconstruction loss (MSE)
  lambda_recon: 1.0

  # Perceptual loss (optional, helps quality)
  # DISABLED temporarily to save memory (VGG network is memory-intensive)
  use_perceptual_loss: false
  lambda_perceptual: 0.1
  perceptual_every_n_steps: 10

  # MS-SSIM loss (helps preserve structure)
  use_ms_ssim_loss: false
  lambda_ssim: 0.1
  ssim_every_n_steps: 10

# Hardware Configuration
hardware:
  device: 'cuda'
  num_gpus: 1
  distributed: false

  # Memory optimization
  gradient_checkpointing: true
  cpu_offload: false

# Expected Performance (Custom VAE Training):
#
# 1. TRAINING SPEED:
#    - Epoch time: ~3-5 minutes (356 patients, batch_size=2)
#    - Total training: ~1-2 hours (20 epochs)
#    - Much faster than diffusion training!
#
# 2. TARGET METRICS:
#    - Epoch 1: PSNR ~25-28 dB (random init)
#    - Epoch 10: PSNR ~35-38 dB (good quality)
#    - Epoch 20: PSNR ~38-42 dB (excellent)
#    - SSIM: 0.90-0.95
#
# 3. CONVERGENCE:
#    - VAEs converge much faster than diffusion models
#    - Should see good results by epoch 10
#    - Can stop early if PSNR >35 dB achieved
#
# 4. NEXT STEPS:
#    - Train VAE first (1-2 hours)
#    - Validate reconstruction quality
#    - Then freeze VAE and train diffusion model
